# -*- coding: utf-8 -*-
"""NAPLAN_Survey_2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NPfZgbbM2mPkVOlpW13sUT7CXAFdD744

# **NAPLAN Student Survey 2022**

## Data Dictionary

| Column | Description|
| :- | :- |
| feeling_rating | How the student was feeling on the day. 5 stars = Great, 4 stars = Pretty good, 3 stars = OK, 2 stars = Not good, 1 star = Terrible |
| low_star_reason | Reason for a 1 or 2 star rating |
| feeling_day_NAPLAN | How the student was feeling on the day about NAPLAN |
| difficulty | How the student found the numeracy exam |
| preparation | How the student prepared for the exam |
| easier_harder | Was the exam easier or harder than expected |
| impact | Was there anything that impacted student performance during the test |

## Load Packages and Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#Enable plots to be displayed
# %matplotlib inline
#Install necessary packages.
import numpy as np
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
# For handling string
import string
# For performing mathematical operations
import math
# For text analysis
import nltk
nltk.download('stopwords')
import re, string
import nltk
import collections
import spacy
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from collections import Counter
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from gensim.corpora.dictionary import Dictionary
from nltk.corpus import wordnet
from gensim.models.tfidfmodel import TfidfModel
from nltk.collocations import *
import pandas as pd
import numpy as np
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
# For pdf viewing
from IPython.display import set_matplotlib_formats
set_matplotlib_formats('pdf', 'svg')

"""## Import Data from Google Drive

"""

from google.colab import drive

# Mount the drive
drive.mount('/content/drive')

# Direct to Google Drive file path
naplan = pd.read_csv('/content/drive/My Drive/NAPLAN_project/NAPLAN_survey.csv')

"""## Data Exploration

1. Preview and shape of data
2. Null values
3. Distribution of ratings
4. Summary statistics of ratings

**1. Preview and Shape**

*Preview*
"""

#View 10 sample rows of naplan data
naplan.sample(10)

"""*Shape*

149 rows, 8 columns
"""

#Shape of data
print("Shape of NAPLAN Data =>",naplan.shape)

"""**2. Null Values**

- 31 students chose a feeling rating of 3 or more. 
- 6 students didn't answer the last question regarding what impacted them during the exam, or they weren't impacted by anything.
"""

#Search for null values
# Look for null values in intent data
naplan.isnull().sum()

"""**Distribution of Ratings**
- 47% of students selected a rating of 3, meaning they felt "OK".
- 3% of students felt "Great" (5 stars).
- 6.7% of students felt "Terrible" (1 star).
"""

sns.histplot(data=naplan, x="feeling_rating", bins=5, discrete=True, shrink=.8, color="orange")

"""**Summary Statistics of Ratings**"""

#Sumary Statistics
naplan['feeling_rating'].describe()

#Boxplot
sns.boxplot(x="feeling_rating",
            data=naplan);

"""## Text Pre-Processing

1. Word count and visuals
2. Collocations

**1. Word Count and Visuals**

Overall:
- the students who were feeling "Pretty Good" wrote the most for how they were feeling on the day of NAPLAN.
- the students who were feeling "Great" wrote the most for the difficulty of the exam.
- the students who were feeling "Not Great" wrote the most for preparation.
- the students who were feeling "Terrible" wrote the most for what impacted them during the exam.
"""

# Average Word count of how students were feeling on the day of NAPLAN depending on feeling rating.
naplan['word_count'] = naplan['feeling_day_NAPLAN'].apply(lambda x: len(str(x).split()))
print(naplan[naplan['feeling_rating']==5]['word_count'].mean()) #Great
print(naplan[naplan['feeling_rating']==4]['word_count'].mean()) #Pretty Good
print(naplan[naplan['feeling_rating']==3]['word_count'].mean()) #OK
print(naplan[naplan['feeling_rating']==2]['word_count'].mean()) #Not Good
print(naplan[naplan['feeling_rating']==1]['word_count'].mean()) #Terrible

# Plot of word count
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))
naplan_words=naplan[naplan['feeling_rating']==5]['word_count']
ax1.hist(naplan_words,color='indigo')
ax1.set_title('Feeling Great')
naplan_words=naplan[naplan['feeling_rating']==4]['word_count']
ax2.hist(naplan_words,color='darkviolet')
ax2.set_title('Feeling Pretty Good')
fig.suptitle('Word Count by Feeling Rating')
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))
naplan_words=naplan[naplan['feeling_rating']==3]['word_count']
ax1.hist(naplan_words,color='mediumorchid')
ax1.set_title('Feeling OK')
naplan_words=naplan[naplan['feeling_rating']==2]['word_count']
ax2.hist(naplan_words,color='plum')
ax2.set_title('Not Good')
fig.suptitle('Word Count by Feeling Rating')
plt.show()

fig,(ax1)=plt.subplots(1,figsize=(10,4))
naplan_words=naplan[naplan['feeling_rating']==1]['word_count']
ax1.hist(naplan_words,color='slateblue')
ax1.set_title('Feeling Terrible')
fig.suptitle('Word Count by Feeling Rating')
plt.show()

# Average Word count depending on difficulty
naplan['word_count'] = naplan['difficulty'].apply(lambda x: len(str(x).split()))
print(naplan[naplan['feeling_rating']==5]['word_count'].mean()) #Great
print(naplan[naplan['feeling_rating']==4]['word_count'].mean()) #Pretty Good
print(naplan[naplan['feeling_rating']==3]['word_count'].mean()) #OK
print(naplan[naplan['feeling_rating']==2]['word_count'].mean()) #Not Good
print(naplan[naplan['feeling_rating']==1]['word_count'].mean()) #Terrible

# Average Word count depending on preparation
naplan['word_count'] = naplan['preparation'].apply(lambda x: len(str(x).split()))
print(naplan[naplan['feeling_rating']==5]['word_count'].mean()) #Great
print(naplan[naplan['feeling_rating']==4]['word_count'].mean()) #Pretty Good
print(naplan[naplan['feeling_rating']==3]['word_count'].mean()) #OK
print(naplan[naplan['feeling_rating']==2]['word_count'].mean()) #Not Good
print(naplan[naplan['feeling_rating']==1]['word_count'].mean()) #Terrible

# Average Word count depending on impact
naplan['word_count'] = naplan['impact'].apply(lambda x: len(str(x).split()))
print(naplan[naplan['feeling_rating']==5]['word_count'].mean()) #Great
print(naplan[naplan['feeling_rating']==4]['word_count'].mean()) #Pretty Good
print(naplan[naplan['feeling_rating']==3]['word_count'].mean()) #OK
print(naplan[naplan['feeling_rating']==2]['word_count'].mean()) #Not Good
print(naplan[naplan['feeling_rating']==1]['word_count'].mean()) #Terrible

"""**2. Collocations**

Collocations is the identification of two or more words that appear next to each other in sentences most frequently.

Process:
1. Create new variable.
2. Remove capital letters and punctuation (adds no value)
3. Tokenize words (splits sentences up into a list of words)
4. Define stopwords (words like "i" and "the" that have no meaning or use in analysing the text)
5. Add customs stopwords (words that aren't in the online list, but are in the dataset that hold no value).
6. Store the good tokens in its own variable.
7. Begin collocations: bigrams (2 word assocations), trigrams (3 word associations) and quadgrams (4 word associations).

Overall:
- Students felt aggravated, annoyed, tired, anxious, stressed and bored on the day of the exam.
- A lot of students commented that this test doesn't go towards anything.
- When students were asked what impacted their performance during the exam, a lot of students said the lagging app, not remembering basic formulas, a negative mindset, clicking of other students' mouse, creaking floor and the student next to them. 
- In the quadgrams, the most common word associations regarded students with runny noses and that their laptops impacted their performance.
- Students also said the halfway time announcement really impacted their performance.

*How students were feeling on the day*
"""

# Create new variable to store clean tokens
text = naplan['feeling_day_NAPLAN'].to_string()

# Remove digits
pattern_order = r'[0-9]'
text= re.sub(pattern_order, '', text)

#Convert to lowercase and remove punctuation
text_lower = text.lower().rstrip(string.punctuation)

#Tokenize words
tokenized_words = word_tokenize(text_lower)
print(tokenized_words)

# Defining stop words
stop_words = stopwords.words("english")
print(stop_words)

#Add custom stop words
stop_words.extend(['nan','really','th', 'le', "n't", "'m", '...','get', ",", ".", 'h', 'r', 'befo', "'ove", "'", ')', 'â€™','-','%', '!', "'ve", "'d", "'st"])

# Store filtered tokens
filtered_tokens= []
for w in tokenized_words:
  if w not in stop_words:
    filtered_tokens.append(w)

print(filtered_tokens)

# Begin collocations process by defining bigram, trigram and quadgram measures
from nltk.collocations import *
bigram_measures = nltk.collocations.BigramAssocMeasures()
trigram_measures = nltk.collocations.TrigramAssocMeasures()
quadgram_measures = nltk.collocations.QuadgramAssocMeasures()
finder = BigramCollocationFinder.from_words(filtered_tokens)

#Beginning with Bigrams, using PMI scores to quantify and rank the BiGrams
finder.nbest(bigram_measures.pmi, 10)

# Import the BigramAssocMeasures to apply to data
from nltk import BigramAssocMeasures
bigram_measures = BigramAssocMeasures()
finder = BigramCollocationFinder.from_words(filtered_tokens)

finder.nbest(bigram_measures.likelihood_ratio, 10)

# Import the TrigramAssocMeasures to apply to data
from nltk import TrigramAssocMeasures
trigram_measure = TrigramAssocMeasures()
finder_tri = TrigramCollocationFinder.from_words(filtered_tokens)
finder_tri.nbest(trigram_measures.likelihood_ratio, 10)

# Import the QuadgramAssocMeasures to apply to data
from nltk import QuadgramAssocMeasures
quadgram_measures = nltk.collocations.QuadgramAssocMeasures()
finder_quad = QuadgramCollocationFinder.from_words(filtered_tokens)
finder_quad.nbest(quadgram_measures.likelihood_ratio, 10)

"""*What students said impacted them*"""

# Create new variable to store clean tokens
impact = naplan['impact'].to_string()

# Remove digits
pattern_order = r'[0-9]'
impact = re.sub(pattern_order, '', impact)

#Convert to lowercase and remove punctuation
impact_lower = impact.lower().rstrip(string.punctuation)

#Tokenize words
tok_words = word_tokenize(impact_lower)
print(tok_words)

#Since stop words have been defined for Intent data, we can skip that step. 
# Store filtered tokens
trust_tokens= []
for w in tok_words:
  if w not in stop_words:
    trust_tokens.append(w)
# Search through data for any other stop words to add to custom list above
print(trust_tokens)

# Bigram, Trigram and Quadgram measures are already loaded. 
finder_trust = BigramCollocationFinder.from_words(trust_tokens)

#Beginning with Bigrams, using PMI scores to quantify and rank the BiGrams
finder_trust.nbest(bigram_measures.pmi, 50)

# Apply Bigrams measure to data
finder_trust = BigramCollocationFinder.from_words(trust_tokens)

finder_trust.nbest(bigram_measures.likelihood_ratio, 20)

# Apply Trigram measure to data
finder_tri_trust = TrigramCollocationFinder.from_words(trust_tokens)
finder_tri_trust.nbest(trigram_measures.likelihood_ratio, 20)

# Apply quadgram measure to data
finder_quad_trust = QuadgramCollocationFinder.from_words(trust_tokens)
finder_quad_trust.nbest(quadgram_measures.likelihood_ratio, 20)

"""## Visualisations

1. Word Cloud
2. Graph of Word Frequency

**1. Word Cloud**
"""

# Create a wordcloud showing most common words for intent data
wordcloud= WordCloud(max_words=100).generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# Create a wordcloud showing most common words for intent data
wordcloud= WordCloud(max_words=100).generate(impact)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# Create a frequency distribution of the most common words in feeling_day_NAPLAN
feeling_words = filtered_tokens
fd = FreqDist(feeling_words)

fd.most_common(10)

# Create a frequency distribution of the most common words in impact
impact_words = trust_tokens
fd_impact = FreqDist(impact_words)

fd_impact.most_common(10)

# Plot top 20 words feeling_day_NAPLAN
top_10 = fd.most_common(20)

# Create a pandas series for a nicer plot
fdist = pd.Series(dict(top_10))

sns.set_theme(style="ticks")

sns.barplot(y=fdist.index, x=fdist.values, color='lightcoral')

# Plot top 20 words feeling_day_NAPLAN
top_10 = fd_impact.most_common(20)

# Create a pandas series for a nicer plot
fdist = pd.Series(dict(top_10))

sns.set_theme(style="ticks")

sns.barplot(y=fdist.index, x=fdist.values, color='yellowgreen')

"""##Sentiment Analysis

Sentiment Analysis of how student was feeling on the day of NAPLAN.

Steps:
1. Apply data cleaning and text pre-processing to new column, clean_text in naplan dataset.
2. Install VADAR (sentiment analysis package)
3. Create new dataframe and store clean_text.
4. Calculate polarity (the overall sentiment of the text)
5. Split up polarity column into negative, neutral, positive and compound columns.
6. Tidy these columns.
7. Create new column for overall sentiment.
8. Identify most positive and most negative statements.
9. Visualize sentiment.


Overall:
- The most positive statement was "great fun".
- The most negative statement was "stressed worry nervous panic nearly cried test".
- 60% of students' feelings on the day of NAPLAN had a negative sentiment.
- 36% of students' feelings on the day of NAPLAN had a positive sentiment. 
"""

# Convert to lowercase, strip and remove punctuations and special characters.
def preprocess(text):
    text = text.lower() 
    text=text.strip()  
    text=re.compile('<.*?>').sub('', text) 
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  
    text = re.sub('\s+', ' ', text)  
    text = re.sub(r'\[[0-9]*\]',' ',text) 
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())
    text = re.sub(r'\d',' ',text) 
    text = re.sub(r'\s+',' ',text) 
    return text

# Stopword removal
def stopword(string):
    a= [i for i in string.split() if i not in stopwords.words('english')]
    return ' '.join(a)

# Lemmatization
# Initialize the lemmatizer
wl = WordNetLemmatizer()
 
# This is a helper function to map NTLK position tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

# Tokenize the sentence
def lemmatizer(string):
    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token
    return " ".join(a)

def finalpreprocess(string):
    return lemmatizer(stopword(preprocess(string)))

#Create new column 'clean text' and apply above objects.
# Apply to intent dataset
naplan['clean_text'] = naplan['feeling_day_NAPLAN'].apply(lambda x: finalpreprocess(x))
naplan.head()

#Store clean_text into dataframe df
df = pd.DataFrame(naplan['clean_text'])
df.head()

# Import VADAR package
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

# Create Sentiment analysis object
analyzer = SentimentIntensityAnalyzer()

# Create new column 'polarity', which will be the positive/negative/neutral score.
df['polarity']= df['clean_text'].apply(lambda x: analyzer.polarity_scores(x))
df.sample(30)

# Change the data structure so each polarity type has its own column
df[['negative', 'neutral', 'positive', 'compound']] = df['polarity'].apply(lambda x: pd.Series(str(x).split(",")))

df.head(10)

#Clean columns
df['negative'] = df['negative'].str.lstrip("{'neg': ")
df['neutral'] = df['neutral'].str.lstrip("'neu': ")
df['positive'] = df['positive'].str.lstrip("'pos': ")
df['compound'] = df['compound'].str.lstrip("'compound': ")
df['compound'] = df['compound'].str.rstrip("}")

df.sample(10)

# Convert polarity columns to floats.
df['negative']=df['negative'].astype(float)
df['neutral']=df['neutral'].astype(float)
df['positive']=df['positive'].astype(float)
df['compound']=df['compound'].astype(float)

# Create a new column 'Sentiment' which displays the overall sentiment depending on the polarity score.
df['sentiment'] = df['compound'].apply(lambda x: 'positive' if x >0 else 'neutral' if x==0 else 'negative')
df.head(10)

# Analysis of results
# Sentence wtih the highest positive sentiment
df.loc[df['compound'].idxmax()].values

# Sentence with the lowest negative sentiment
df.loc[df['compound'].idxmin()].values

# Plot of sentiment count
sns.countplot(y='sentiment', 
             data=df, 
             palette=['#db3d13', "#008080",'#b2d8d8']
             );

# Boxplot
sns.boxplot(y='compound', 
            x='sentiment',
            palette=['#db3d13',"#008080",'#b2d8d8'], 
            data=df);

"""# **Congratulations! You've made it to the end.**"""